{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "data_location = Path(\"../Data/Tableau_Terra_workspace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the terra workspace \n",
    "Read all these weird files and try to merge them to the best of my capabilities:\n",
    "\n",
    "1. Read and merge CLF with Premium\n",
    "2. Read and merge Panel data\n",
    "3. Read RNA data\n",
    "4. Read and merge WES data - UPCOMMING\n",
    "5. Apply loop through rows\n",
    "6. Duplicate rows and add conditions\n",
    "7. Check number of datapoints\n",
    "\n",
    "Note, losing alot of information due to inconsistencies in the datasets. Another note this workflow eats up memory, because I keep meta data open, just something to note\n",
    "\n",
    "### 1 Read and merge CLF with Premium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../Data/Tableau_Terra_workspace/CLF_Cohort_Summary_full_data.csv' does not exist: b'../Data/Tableau_Terra_workspace/CLF_Cohort_Summary_full_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-80a2150c67dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#CLF data and cleaning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mCLF_data_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_location\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"CLF_Cohort_Summary_full_data.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mCLF_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLF_data_meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Cell Line\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PT-ID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SM-ID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Status\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Resolution\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"T/N\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Primary Disease\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tissue site\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tumor type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CELL_LINE_ID(raw)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Media type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Growth Properties\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Flask coating\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Starting Material Type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Fail Mode\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mCLF_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLF_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'^\\s*$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mCLF_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLF_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Cell Line\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Cell Line Starting Sample\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tissue site\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Tissue Site\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tumor type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Tumor Type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CELL_LINE_ID(raw)\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"Cell Line\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Growth Properties\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dimension\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitProjects/virtualenv/Broadenv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitProjects/virtualenv/Broadenv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitProjects/virtualenv/Broadenv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitProjects/virtualenv/Broadenv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitProjects/virtualenv/Broadenv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../Data/Tableau_Terra_workspace/CLF_Cohort_Summary_full_data.csv' does not exist: b'../Data/Tableau_Terra_workspace/CLF_Cohort_Summary_full_data.csv'"
     ]
    }
   ],
   "source": [
    "#CLF data and cleaning\n",
    "CLF_data_meta = pd.read_csv(data_location / \"CLF_Cohort_Summary_full_data.csv\", encoding = 'utf-8', sep='\\t')\n",
    "CLF_data = CLF_data_meta[[\"Cell Line\", \"PT-ID\", \"SM-ID\", \"Status\", \"Resolution\", \"T/N\", \"Primary Disease\", \"Tissue site\", \"Tumor type\", \"CELL_LINE_ID(raw)\", \"Media type\", \"Growth Properties\", \"Flask coating\", \"Starting Material Type\", \"Fail Mode\"]]\n",
    "CLF_data = CLF_data.replace(r'^\\s*$', np.nan, regex=True)\n",
    "CLF_data = CLF_data.rename(columns={\"Cell Line\": \"Cell Line Starting Sample\", \"Tissue site\": \"Tissue Site\", \"Tumor type\": \"Tumor Type\", \"CELL_LINE_ID(raw)\":\"Cell Line\", \"Growth Properties\": \"Dimension\"})\n",
    "CLF_data[\"Dataset\"] = \"CLF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Premium data and cleaning\n",
    "Premium_data_meta = pd.read_csv(data_location / \"Premium_Cohort_Summary_full_data.csv\", encoding = 'utf-8', sep='\\t')\n",
    "Premium_data = Premium_data_meta[[\"Cell Line Starting Sample\", \"PT_ID\", \"Passage Sample\", \"JIRA Status\", \"Resolution\", \"T/N\", \"Primary disease\", \"Tissue Site\", \"Tumor Type\", \"CELL_LINE_ID\", \"Growth Medium\", \"Growth Pattern\", \"ECM Bio Coatings\", \"Root Material Type\", \"Passage Sample Status\"]]\n",
    "Premium_data = Premium_data.replace(r'^\\s*$', np.nan, regex=True)\n",
    "Premium_data = Premium_data.rename(columns={\"PT_ID\": \"PT-ID\", \"Passage Sample\": \"SM-ID\", \"JIRA Status\": \"Status\", \"Primary disease\": \"Primary Disease\", \"CELL_LINE_ID\":\"Cell Line\", \"Growth Medium\":\"Media type\", \"Growth Pattern\":\"Dimension\", \"ECM Bio Coatings\":\"Flask coating\", \"Root Material Type\":\"Starting Material Type\", \"Passage Sample Status\":\"Fail Mode\"})\n",
    "Premium_data[\"Dataset\"] = \"Premium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge CLF and Premium\n",
    "data = pd.concat([CLF_data, Premium_data])\n",
    "\n",
    "# Remove Normals\n",
    "data = data[data[\"T/N\"] == \"Tumor\"]\n",
    "\n",
    "# Cleanup some of columns\n",
    "data[\"SM-ID\"] = [re.split('\\r|  ',x)[0].strip(' \\t\\n\\r') if str(x) != 'nan' else x for x in list(data[\"SM-ID\"])]\n",
    "data[\"Resolution\"] = [str(x) for x in list(data[\"Resolution\"])]\n",
    "data[\"Cell Line\"] = [str(x) for x in list(data[\"Cell Line\"])]\n",
    "data[\"Cell Line\"] = [re.split('\\r|  ',x)[0].strip(' \\t\\n\\r') if str(x) != 'nan' else x for x in list(data[\"Cell Line\"])]\n",
    "data[\"Media type\"] = [str(x) for x in list(data[\"Media type\"])]\n",
    "data = data[data[\"Media type\"] != \"nan\"]\n",
    "\n",
    "# Note Dropping lot of conditions\n",
    "Resolution_dict = {\n",
    "    'Terminated':'Terminated',\n",
    "    'In Process':'In Process',\n",
    "    'Verification':'In Process',\n",
    "    'Ambiguous':'Drop',\n",
    "    'Completed':'Completed',\n",
    "    'Verified Normal':'Verified Normal',\n",
    "    'Verified Tumor':'Verified Tumor',\n",
    "    'Clinical team follow up':'In Process',\n",
    "    'Other verified tumor models':'Verified Tumor',\n",
    "    'Primary sample verified normal':'Verified Normal',\n",
    "    'Blacklisted; FP Failure':'Drop',\n",
    "    'Low Purity Tumor':'Drop',\n",
    "    'Clinical team follow up,Primary sample verified normal':'Verified Normal',\n",
    "    'Clinical team follow up,Other verified tumor models':'Verified Tumor',\n",
    "    'nan':'nan',\n",
    "    'Abandoned':'Drop',\n",
    "}\n",
    "\n",
    "data[\"Resolution_modified\"] = data[\"Resolution\"]\n",
    "data = data.replace({\"Resolution_modified\": Resolution_dict})\n",
    "data = data[(data[\"Resolution_modified\"] != \"Drop\") & (data[\"Resolution_modified\"] != \"In Process\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Read and combined Panel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../Data/Tableau_Terra_workspace/PANCAN_TWIST_sample.tsv' does not exist: b'../Data/Tableau_Terra_workspace/PANCAN_TWIST_sample.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-dde66bea5383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TWIST Panel data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTWIST_samples_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_location\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"PANCAN_TWIST_sample.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mTWIST_pair_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_location\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"PANCAN_TWIST_pair.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mTWIST_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTWIST_samples_meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entity:sample_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"external_id_validation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"participant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"patient_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sample_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"renamed_bai_file\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"renamed_bam_file\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"depth_of_cov_qc_result\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cnv_calls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"media\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mTWIST_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tn_decision_clean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Not Reported\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitProjects/virtualenv/Broadenv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitProjects/virtualenv/Broadenv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitProjects/virtualenv/Broadenv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitProjects/virtualenv/Broadenv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitProjects/virtualenv/Broadenv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../Data/Tableau_Terra_workspace/PANCAN_TWIST_sample.tsv' does not exist: b'../Data/Tableau_Terra_workspace/PANCAN_TWIST_sample.tsv'"
     ]
    }
   ],
   "source": [
    "# TWIST Panel data\n",
    "TWIST_samples_meta = pd.read_csv(data_location / \"PANCAN_TWIST_sample.tsv\", sep=\"\\t\")\n",
    "TWIST_pair_meta = pd.read_csv(data_location / \"PANCAN_TWIST_pair.tsv\", sep=\"\\t\")\n",
    "TWIST_samples = TWIST_samples_meta[[\"entity:sample_id\", \"external_id_validation\", \"participant\", \"patient_id\", \"sample_type\", \"renamed_bai_file\", \"renamed_bam_file\", \"depth_of_cov_qc_result\", \"cnv_calls\", \"media\"]]\n",
    "TWIST_samples['tn_decision_clean'] = \"Not Reported\"\n",
    "TWIST_pair = TWIST_pair_meta[[\"case_sample\", \"filtered_variants\", \"mutect1_callstats\", \"mutect1_coveragewig\", \"mutect1_powerwig\", \"mutect1_vcf\", \"mutect2_vcf\", \"oncotated_maf_mutect1\", \"oncotated_maf_mutect2\"]]\n",
    "\n",
    "TWIST_samples = TWIST_samples[TWIST_samples[\"entity:sample_id\"].isin(TWIST_pair[\"case_sample\"])]\n",
    "TWIST_samples = TWIST_samples.rename(columns={\"entity:sample_id\":\"case_sample\"})\n",
    "TWIST = pd.merge(TWIST_samples, TWIST_pair, on='case_sample')\n",
    "TWIST = TWIST.replace(r'^\\s*$', np.nan, regex=True)\n",
    "TWIST[\"Dataset\"] = \"TWIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../Data/Tableau_Terra_workspace/TSCA_HCMI_sample.tsv' does not exist: b'../Data/Tableau_Terra_workspace/TSCA_HCMI_sample.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-08908d15ae7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TSCA Panel data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTSCA_samples_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_location\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"TSCA_HCMI_sample.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mTSCA_pair_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_location\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"TSCA_HCMI_pair.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mTSCA_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSCA_samples_meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entity:sample_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bsp_sample_id_validation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"external_id_validation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"participant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sample_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"renamed_bai_file\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"renamed_bam_file\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"depth_of_cov_qc_result\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cnv_calls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"media\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tn_decision_clean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mTSCA_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSCA_pair_meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"case_sample\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"filtered_variants\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mutect1_callstats\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mutect1_coveragewig\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mutect1_powerwig\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mutect1_vcf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mutect2_vcf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"oncotated_maf_mutect1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"oncotated_maf_mutect2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitProjects/virtualenv/Broadenv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitProjects/virtualenv/Broadenv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitProjects/virtualenv/Broadenv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitProjects/virtualenv/Broadenv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitProjects/virtualenv/Broadenv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../Data/Tableau_Terra_workspace/TSCA_HCMI_sample.tsv' does not exist: b'../Data/Tableau_Terra_workspace/TSCA_HCMI_sample.tsv'"
     ]
    }
   ],
   "source": [
    "# TSCA Panel data\n",
    "TSCA_samples_meta = pd.read_csv(data_location / \"TSCA_HCMI_sample.tsv\", sep=\"\\t\")\n",
    "TSCA_pair_meta = pd.read_csv(data_location / \"TSCA_HCMI_pair.tsv\", sep=\"\\t\")\n",
    "TSCA_samples = TSCA_samples_meta[[\"entity:sample_id\", \"bsp_sample_id_validation\", \"external_id_validation\", \"participant\", \"sample_type\", \"renamed_bai_file\", \"renamed_bam_file\", \"depth_of_cov_qc_result\", \"cnv_calls\", \"media\", \"tn_decision_clean\"]]\n",
    "TSCA_pair = TSCA_pair_meta[[\"case_sample\", \"filtered_variants\", \"mutect1_callstats\", \"mutect1_coveragewig\", \"mutect1_powerwig\", \"mutect1_vcf\", \"mutect2_vcf\", \"oncotated_maf_mutect1\", \"oncotated_maf_mutect2\"]]\n",
    "TSCA_pair = TSCA_pair.dropna()\n",
    "\n",
    "TSCA_samples = TSCA_samples[TSCA_samples[\"entity:sample_id\"].isin(TSCA_pair[\"case_sample\"])]\n",
    "TSCA_samples = TSCA_samples.rename(columns={\"entity:sample_id\":\"case_sample\"})\n",
    "TSCA = pd.merge(TSCA_samples, TSCA_pair, on='case_sample')\n",
    "TSCA = TSCA.drop(columns=[\"case_sample\"])\n",
    "TSCA = TSCA.rename(columns={\"bsp_sample_id_validation\":\"case_sample\"})\n",
    "TSCA = TSCA.replace(r'^\\s*$', np.nan, regex=True)\n",
    "TSCA[\"Dataset\"] = \"TSCA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TWIST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-559530e4b5e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Merge TWIST and TSCA with Tableau_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mPanel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTWIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTSCA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Adding PANEL column head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPanel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPanel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"patient_id\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"PT-ID\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TWIST' is not defined"
     ]
    }
   ],
   "source": [
    "# Merge TWIST and TSCA with Tableau_data\n",
    "Panel = pd.concat([TWIST, TSCA])\n",
    "\n",
    "#Adding PANEL column head\n",
    "Panel = Panel.rename(columns={\"patient_id\":\"PT-ID\"})\n",
    "Panel.columns = [\"PANEL_\" + x if i > 4 else x for i,x in enumerate(Panel.columns)]\n",
    "\n",
    "Panel[\"participant\"] = [str(x) for x in Panel[\"participant\"]]\n",
    "Panel[\"PANEL_tn_decision_clean\"] = [str(x) for x in Panel[\"PANEL_tn_decision_clean\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Read RNA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNA data\n",
    "RNA_meta = pd.read_csv(data_location / \"CCLF_RNA_2_0.tsv\", sep=\"\\t\")\n",
    "RNA = RNA_meta[[\"entity:sample_id\", \n",
    "                     \"bsp_sample_id_rna\",  \n",
    "                     \"participant\", \n",
    "                     \"sample_type\", \n",
    "                     \"fastq1\", \n",
    "                     \"fastq2\", \n",
    "                     \"bai_file_rna\", \n",
    "                     \"bam_file_rna\", \n",
    "                     \"fusion_predictions\", \n",
    "                     \"fusion_predictions_abridged\", \n",
    "                     \"rnaseqc_count_metrics\", \n",
    "                     \"rnaseqc_count_outputs\", \n",
    "                     \"rnaseqc_exon_counts\", \n",
    "                     \"rnaseqc_gene_counts\", \n",
    "                     \"rnaseqc_gene_rpkm\", \n",
    "                     \"rsem_genes\", \n",
    "                     \"rsem_isoforms\", \n",
    "                     \"star_bam_file\", \n",
    "                     \"star_bam_index\", \n",
    "                     \"star_chimeric_bam_index\", \n",
    "                     \"star_chimeric_junctions\", \n",
    "                     \"star_junctions\", \n",
    "                     \"star_junctions_pass1\", \n",
    "                     \"star_md_bam_file\", \n",
    "                     \"star_md_bam_index\", \n",
    "                     \"star_read_counts\",\n",
    "                    \"star_transcriptome_bam\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding RNA column head\n",
    "RNA.columns = [\"RNA_\" + x if i > 3 else x for i,x in enumerate(RNA.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-80-0eccdc508b84>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  RNA[\"bsp_sample_id_rna\"] = [str(x) for x in RNA[\"bsp_sample_id_rna\"]]\n",
      "<ipython-input-80-0eccdc508b84>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  RNA[\"participant\"] = [str(x) for x in RNA[\"participant\"]]\n"
     ]
    }
   ],
   "source": [
    "# Removing without SM-ID\n",
    "total_rows = len(RNA)\n",
    "RNA[\"bsp_sample_id_rna\"] = [str(x) for x in RNA[\"bsp_sample_id_rna\"]]\n",
    "RNA[\"participant\"] = [str(x) for x in RNA[\"participant\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Read WES data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WES data\n",
    "WES_samples_meta = pd.read_csv(data_location / \"CCLF_WES_sample.tsv\", sep=\"\\t\")\n",
    "#WES_pair_meta = pd.read_csv(data_location / \"CCLF_WES_pair.tsv\", sep=\"\\t\")\n",
    "WES_samples = WES_samples_meta[[\"entity:sample_id\", \"bsp_sample_id_capture\", \"external_id_capture\", \"participant\", \"sample_type\", \"formatted_bai_file\", \"formatted_bam_file\", \"cnv_calls\"]]\n",
    "\n",
    "# Pair is wierd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "WES = WES_samples\n",
    "WES.columns = [\"WES_\" + x if i > 4 else x for i,x in enumerate(WES.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Loop through data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def growth_check(x):\n",
    "    mapping_dict = {\n",
    "        'Terminated No Growth': 0,\n",
    "        'Terminated Unlisted': 0,\n",
    "        'Terminated Fungus Contamination': 0,\n",
    "        'Terminated Stopped Growing': 0,\n",
    "        'Terminated Fungus Contamination,No Growth': 0,\n",
    "        'Terminated Bacterial Contamination': 0,\n",
    "        'Terminated Bacterial Contamination,No Growth': 0,\n",
    "        'Terminated Bacterial Contamination,Fungus Contamination,Sample Quality': 0,\n",
    "        'Terminated Fibroblast Overgrowth': 0,\n",
    "        'Terminated Feeder Contamination,No Growth': 0,\n",
    "        'Verified Normal nan': 0,\n",
    "        'Verified Normal Fibroblast Overgrowth': 0,\n",
    "        'Verified Tumor nan': 1,\n",
    "        'Verified Tumor Stopped Growing': 1,\n",
    "        'Terminated Feeder Contamination': 0,\n",
    "        'Verified Tumor Fingerprinting Failure': 2,\n",
    "        'Verified Tumor Unlisted': 0,\n",
    "        'Verified Normal Mouse Contamination': 0,\n",
    "        'Verified Normal Stopped Growing': 0,\n",
    "        'Verified Normal Unlisted': 0,\n",
    "        'Verified Normal Sample Purity': 0,\n",
    "        'Terminated Fingerprinting Failure': 2,\n",
    "        'Terminated Bacterial Contamination,Fungus Contamination': 0,\n",
    "        'Terminated Bacterial Contamination,Fungus Contamination,No Growth': 0,\n",
    "        'Terminated No Growth,Sample Quality': 0,\n",
    "        'Terminated Sample Quality': 0,\n",
    "        'Terminated Lab Error,Sample Purity': 0,\n",
    "        'Terminated Mouse Contamination': 0,\n",
    "        'Terminated Fibroblast Overgrowth,Mouse Contamination': 0,\n",
    "        'Verified Tumor No Growth': 0,\n",
    "        'Terminated No Growth After Thaw': 0,\n",
    "        'Terminated No Growth,Sample Purity,Sample Quality': 0,\n",
    "        'Terminated Lab Error': 0,\n",
    "        'Terminated Fingerprinting Failure,Lab Error': 0,\n",
    "        'Verified Normal No Growth': 0,\n",
    "        'Terminated No Growth,Sample Purity': 0,\n",
    "        'Terminated Sample Purity': 0,\n",
    "        'Verified Tumor Terminated': 0,\n",
    "        'Terminated Terminated': 0,\n",
    "        'Terminated Received/In Container': 0,\n",
    "        'Completed Terminated': 0,\n",
    "        'Completed Received/Out of Storage': 0,\n",
    "        'Terminated nan': 0,\n",
    "        'nan nan': 2,\n",
    "        'Verified Tumor Received/Out of Storage': 2,\n",
    "        'Verified Tumor Received/In Container': 1,\n",
    "        'Verified Normal Terminated': 0,\n",
    "        'Verified Normal Received/In Container': 0,\n",
    "        'Verified Normal Received/Out of Storage': 0,\n",
    "        'nan Terminated': 0,\n",
    "        'nan Received/In Container': 2,\n",
    "        'Terminated Received/Out of Storage': 0,\n",
    "        'nan Received/Out of Storage': 2,\n",
    "        'Completed nan': 1,\n",
    "    }\n",
    "    \n",
    "    # Checking resolution and mapping back\n",
    "    x[\"Fail Mode\"] = [str(x) for x in x[\"Fail Mode\"]]\n",
    "    \n",
    "    series = x[\"Resolution_modified\"] + \" \" + x[\"Fail Mode\"]\n",
    "    series = [str(x) for x in series]\n",
    "\n",
    "    x[\"Growth\"] = series\n",
    "    x = x.replace({\"Growth\": mapping_dict})\n",
    "\n",
    "    x = x[x[\"Growth\"] != 2]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = growth_check(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 822, 0: 10879})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(data[\"Growth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes due to https://github.com/broadinstitute/ml_cclf/blob/master/data_exploration/12042019_update_data/premium_to_standard_format.ipynb\n",
    "- CCLF_RCRF1025T => 64 conditions growing\n",
    "- CCLF_RCRF1033T => 36 conditions growing\n",
    "- EW012T => 36 conditions growing\n",
    "- BID011 - 36 conditions growing\n",
    "- CCLF_KL1235T => 16 conditions growing\n",
    "- CCLF_RCRF1049T => All 16 conditions growing\n",
    "- CCLF_RCRF1058T => All 16 conditions growing\n",
    "- CCLF_RCRF1061T => All 16 conditions growing\n",
    "- CCLF_BU1017T => All 16 conditions growing\n",
    "\n",
    "CCLF_RCRF1009T:\n",
    "- RETM, SMGM\n",
    "- AR5, RETM\n",
    "- AR5, CM\n",
    "\n",
    "<br />\n",
    "\n",
    "CCLF_RCRF1032T:\n",
    "- HT Media Screen\n",
    "\n",
    "<br />\n",
    "\n",
    "BID018_ASC:\n",
    "- BID018_ASC_AR5_p5_3D\n",
    "- RETM_AR5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_growth_media(growth_media: pd.Series, *media_list):\n",
    "    \"\"\"Function to return if the growth_media contains one of the media in media_list\n",
    "    (Media is plural, e.g.: 'AR5', 'RETM')\n",
    "    \"\"\"\n",
    "    # If only one media in the media list, we should handle that\n",
    "    only_one_medium_list = []\n",
    "    for media in media_list:\n",
    "        if len(media) == 1:\n",
    "            # This media should go out before we process the others\n",
    "            only_one_medium_list.append(media[0])\n",
    "    \n",
    "    # Handle unique media using % char => Indicates how many media\n",
    "    are_medium_in_growth_media = len(growth_media.split('%')) == 2 and\\\n",
    "                               any(medium in growth_media\n",
    "                                  for medium in only_one_medium_list) \n",
    "    \n",
    "    are_media_in_growth_media = any(all(medium in growth_media\n",
    "                                        for medium in current_media)\n",
    "                                    for current_media in media_list\n",
    "                                   if current_media not in only_one_medium_list)\n",
    "    \n",
    "    return are_media_in_growth_media | are_medium_in_growth_media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EW012T: Only\n",
    "# TODO: Take into account some are VN\n",
    "def terminated_vt_not_growing(tmp, current_cell_line, tuple_media_list):\n",
    "    condition_not_cell_line = tmp['Cell Line'] != current_cell_line\n",
    "    condition_cell_line = tmp['Cell Line'] == current_cell_line\n",
    "    \n",
    "    condition_growing_media = tmp['Media type']\\\n",
    "                .apply(filter_growth_media,\n",
    "                          args=tuple_media_list)\n",
    "    condition_cell_line_no_grow = (tmp['Cell Line'] == current_cell_line) & \\\n",
    "                            (condition_growing_media)\n",
    "\n",
    "    data.loc[data[condition_cell_line_no_grow].index,\\\n",
    "                         'Growth'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change some weird not growing samples\n",
    "terminated_vt_not_growing(data, 'CCLF_RCRF1025T',\n",
    "                         (['SMGM', 'RETM'],\n",
    "                           ['AR5', 'RETM']))\n",
    "\n",
    "terminated_vt_not_growing(data, 'CCLF_RCRF1033T',\n",
    "                         (['M87', 'AR5'],\n",
    "                          ['RETM', 'WIT-P'],\n",
    "                          ['SMGM', 'M87']))\n",
    "\n",
    "terminated_vt_not_growing(data, 'EW012T',\n",
    "                         (['RETM', 'AR5'],))\n",
    "\n",
    "terminated_vt_not_growing(data, 'BID011',\n",
    "                         (['CM'],))\n",
    "\n",
    "terminated_vt_not_growing(data, 'CCLF_KL1235T',\n",
    "                         (['AR5', 'OPAC'],))\n",
    "\n",
    "terminated_vt_not_growing(data, 'CCLF_RCRF1049T',\n",
    "                         (['AR5', 'EGM'],))\n",
    "\n",
    "terminated_vt_not_growing(data, 'CCLF_RCRF1058T',\n",
    "                         (['RETM', 'AR5'], ['RETM', 'M87'],\n",
    "                         ['SMGM', 'M87'], ['RETM', 'SMGM']))\n",
    "\n",
    "terminated_vt_not_growing(data, 'CCLF_RCRF1061T',\n",
    "                         (['BEGM', 'OPAC'], ['RETM', 'OPAC'],\n",
    "                         ['OPAC'], ['RETM', 'AR5'],\n",
    "                         ['AR5'], ['AR5', 'OPAC']))\n",
    "\n",
    "terminated_vt_not_growing(data, 'CCLF_BU1017T',\n",
    "                         (['AR5', 'OPAC'], ['RETM', 'OPAC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change some others aswell\n",
    "#CCLF_RCRF1009T\n",
    "#CCLF_RCRF1032T\n",
    "#BID018_ASC\n",
    "terminated_vt_not_growing(data, 'CCLF_RCRF1009T',\n",
    "                         (['RETM', 'SMGM'],\n",
    "                           ['AR5', 'RETM'],\n",
    "                           ['AR5', 'CM']))\n",
    "\n",
    "\n",
    "terminated_vt_not_growing(data, 'CCLF_RCRF1032T',\n",
    "                         (['AR5', 'CM'],\n",
    "                           ['CM', 'WIT-P'],\n",
    "                           ['RETM', 'AR5'],\n",
    "                           ['RETM', 'SGMG']))\n",
    "\n",
    "terminated_vt_not_growing(data, 'BID018_ASC',\n",
    "                         (['RETM', 'AR5']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=['Cell Line Starting Sample', \n",
    "                                     'Cell Line', \n",
    "                                     'Primary Disease', \n",
    "                                     'Media type', \n",
    "                                     'Dimension', \n",
    "                                     'Flask coating', \n",
    "                                     'Starting Material Type', \n",
    "                                     'Fail Mode', \n",
    "                                     'Resolution_modified'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_score(x):\n",
    "    Lookup_table = {\n",
    "        \"PRIMARY\":19,\n",
    "        \"P0\":18,\n",
    "        \"P1\":17,\n",
    "        \"P2\":16,\n",
    "        \"P3\":15,\n",
    "        \"P4\":14,\n",
    "        \"P5\":13,\n",
    "        \"P6\":12,\n",
    "        \"P7\":11,\n",
    "        \"P8\":10,\n",
    "        \"P9\":9,\n",
    "        \"P10\":8,\n",
    "        \"P11\":7,\n",
    "        \"P12\":6,\n",
    "        \"P13\":5,\n",
    "        \"P14\":4,\n",
    "        \"P15\":3,\n",
    "        \"P16\":2,\n",
    "        \"P17\":1,\n",
    "    }\n",
    "\n",
    "    test = [x.upper().split(\"_\") for x in x[\"external_id_validation\"]]\n",
    "    \n",
    "    value = []\n",
    "    for t in test:\n",
    "        for key in t:\n",
    "            if key in Lookup_table:\n",
    "                value_tmp = Lookup_table[key]\n",
    "                break\n",
    "            else:\n",
    "                value_tmp = 0\n",
    "\n",
    "        value.append(value_tmp)\n",
    "        \n",
    "    x[\"test\"] = value\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_numpy_panel = np.empty([1,len(list(Panel)[5:])])\n",
    "empty_numpy_panel.fill(np.nan)\n",
    "empty_frame_panel = pd.DataFrame(empty_numpy_panel, columns=list(Panel)[5:])\n",
    "\n",
    "empty_numpy_RNA = np.empty([1,len(list(RNA)[4:])])\n",
    "empty_numpy_RNA.fill(np.nan)\n",
    "empty_frame_RNA = pd.DataFrame(empty_numpy_RNA, columns=list(RNA)[4:])\n",
    "\n",
    "empty_numpy_WES = np.empty([1,len(list(WES)[5:])])\n",
    "empty_numpy_WES.fill(np.nan)\n",
    "empty_frame_WES = pd.DataFrame(empty_numpy_WES, columns=list(WES)[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nan_panel(data_tmp, empty_numpy_panel):\n",
    "    data_tmp = pd.concat([data_tmp.reset_index(drop=True), pd.concat([empty_frame_panel]*len(data_tmp)).reset_index(drop=True)], axis=1)\n",
    "    return(data_tmp)\n",
    "\n",
    "def add_nan_RNA(data_tmp, empty_frame_RNA):\n",
    "    data_tmp = pd.concat([data_tmp.reset_index(drop=True), pd.concat([empty_frame_RNA]*len(data_tmp)).reset_index(drop=True)], axis=1)\n",
    "    return(data_tmp)\n",
    "\n",
    "def add_nan_WES(data_tmp, empty_frame_WES):\n",
    "    data_tmp = pd.concat([data_tmp.reset_index(drop=True), pd.concat([empty_frame_WES]*len(data_tmp)).reset_index(drop=True)], axis=1)\n",
    "    return(data_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-91-c83dd44c1f94>:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x[\"test\"] = value\n"
     ]
    }
   ],
   "source": [
    "spreadsheet = []\n",
    "\n",
    "for i, cell_line in enumerate(data[\"Cell Line\"].unique()):\n",
    "    data_tmp = data[data[\"Cell Line\"] == cell_line]\n",
    "    # At least one that isn't Verified Normal\n",
    "    if all(data_tmp[\"Resolution_modified\"] == \"Verified Normal\") != True:\n",
    "        # First get the right data from Panel/RNA\n",
    "        if cell_line.endswith(\"T\"):\n",
    "            Panel_tmp = Panel[Panel[\"participant\"] == cell_line[:-1]]\n",
    "            RNA_tmp = RNA[RNA[\"participant\"] == cell_line[:-1]]\n",
    "            WES_tmp = WES[WES[\"participant\"] == cell_line[:-1]]\n",
    "        else:\n",
    "            Panel_tmp = Panel[Panel[\"participant\"] == cell_line]\n",
    "            RNA_tmp = RNA[RNA[\"participant\"] == cell_line]\n",
    "            WES_tmp = WES[WES[\"participant\"] == cell_line]\n",
    "\n",
    "        #Add Panel data\n",
    "        if not Panel_tmp.empty:\n",
    "            # Check if all are not VN\n",
    "            if all(Panel_tmp[\"PANEL_tn_decision_clean\"] == \"VN\") != True:\n",
    "                # Check if multiple sequencing results for cell line\n",
    "                if len(Panel_tmp) == 1:\n",
    "                    Panel_tmp = Panel_tmp.iloc[0:, 5:]\n",
    "                    data_tmp = pd.concat([data_tmp.reset_index(drop=True), pd.concat([Panel_tmp]*len(data_tmp)).reset_index(drop=True)], axis=1)\n",
    "                else:\n",
    "                    # Loop through these panels\n",
    "                    Panel_tmp = create_score(Panel_tmp)\n",
    "                    Panel_tmp = Panel_tmp[Panel_tmp[\"test\"] == max(Panel_tmp[\"test\"])]\n",
    "                    if all(Panel_tmp[\"PANEL_tn_decision_clean\"] == \"VN\") != True:\n",
    "                        if len(Panel_tmp) == 1:\n",
    "                            Panel_tmp = Panel_tmp.iloc[0:1, 5:-1]\n",
    "                            data_tmp = pd.concat([data_tmp.reset_index(drop=True), pd.concat([Panel_tmp]*len(data_tmp)).reset_index(drop=True)], axis=1)\n",
    "                        else:\n",
    "                            # Just picking the one which is not VN\n",
    "                            Panel_tmp = Panel_tmp[Panel_tmp[\"PANEL_tn_decision_clean\"] != \"VN\"]\n",
    "                            if not Panel_tmp.empty:\n",
    "                                Panel_tmp_tmp = Panel_tmp[Panel_tmp[\"PANEL_tn_decision_clean\"] == \"VT\"]\n",
    "                                if not Panel_tmp_tmp.empty:\n",
    "                                    Panel_tmp_tmp = Panel_tmp_tmp.iloc[0:1, 5:-1]\n",
    "                                    data_tmp = pd.concat([data_tmp.reset_index(drop=True), pd.concat([Panel_tmp_tmp]*len(data_tmp)).reset_index(drop=True)], axis=1)\n",
    "                                else:\n",
    "                                    Panel_tmp = Panel_tmp.iloc[0:1, 5:-1]\n",
    "                                    data_tmp = pd.concat([data_tmp.reset_index(drop=True), pd.concat([Panel_tmp]*len(data_tmp)).reset_index(drop=True)], axis=1)\n",
    "                            else:\n",
    "                                #Add empty columns\n",
    "                                data_tmp = add_nan_panel(data_tmp, empty_frame_panel)\n",
    "                                \n",
    "                    else:\n",
    "                        #Add empty columns\n",
    "                        data_tmp = add_nan_panel(data_tmp, empty_frame_panel)\n",
    "\n",
    "            else:\n",
    "                #Add empty columns\n",
    "                data_tmp = add_nan_panel(data_tmp, empty_frame_panel)\n",
    "        \n",
    "        else:\n",
    "            #Add empty columns\n",
    "            data_tmp = add_nan_panel(data_tmp, empty_frame_panel)\n",
    "                \n",
    "        #Add RNA data\n",
    "        if not RNA_tmp.empty:\n",
    "            if len(RNA_tmp) == 1:\n",
    "                RNA_tmp = RNA_tmp.iloc[0:1, 4:]\n",
    "                data_tmp = pd.concat([data_tmp.reset_index(drop=True), pd.concat([RNA_tmp]*len(data_tmp)).reset_index(drop=True)], axis=1)\n",
    "            else:\n",
    "                if len(data_tmp) == 1:\n",
    "                    RNA_tmp = RNA_tmp.iloc[0:1, 4:]\n",
    "                    data_tmp = pd.concat([pd.concat([data_tmp]*len(RNA_tmp)).reset_index(drop=True), RNA_tmp.reset_index(drop=True)], axis=1)\n",
    "                else:\n",
    "                    # Just pick one\n",
    "                    RNA_tmp = RNA_tmp.iloc[0:1, 4:]\n",
    "                    data_tmp = pd.concat([data_tmp.reset_index(drop=True), pd.concat([RNA_tmp]*len(data_tmp)).reset_index(drop=True)], axis=1)\n",
    "                        \n",
    "        else:\n",
    "            data_tmp = add_nan_RNA(data_tmp, empty_frame_RNA)\n",
    "                          \n",
    "        #Add WES data\n",
    "        if not WES_tmp.empty:\n",
    "            if len(WES_tmp) == 1:\n",
    "                WES_tmp = WES_tmp.iloc[0:1, 5:]\n",
    "                data_tmp = pd.concat([data_tmp.reset_index(drop=True), pd.concat([WES_tmp]*len(data_tmp)).reset_index(drop=True)], axis=1)\n",
    "            else:\n",
    "                if len(data_tmp) == 1:\n",
    "                    WES_tmp = WES_tmp.iloc[0:1, 5:]\n",
    "                    data_tmp = pd.concat([pd.concat([data_tmp]*len(WES_tmp)).reset_index(drop=True), WES_tmp.reset_index(drop=True)], axis=1)\n",
    "                else:\n",
    "                    # Just pick one\n",
    "                    WES_tmp = WES_tmp.iloc[0:1, 5:]\n",
    "                    data_tmp = pd.concat([data_tmp.reset_index(drop=True), pd.concat([WES_tmp]*len(data_tmp)).reset_index(drop=True)], axis=1)\n",
    "                        \n",
    "        else:\n",
    "            data_tmp = add_nan_WES(data_tmp, empty_frame_WES)\n",
    "        \n",
    "    else:\n",
    "        #Add empty columns\n",
    "        data_tmp = add_nan_panel(data_tmp, empty_frame_panel)\n",
    "        data_tmp = add_nan_RNA(data_tmp, empty_frame_RNA)\n",
    "        data_tmp = add_nan_WES(data_tmp, empty_frame_WES)\n",
    "        \n",
    "    data_tmp[\"RNA_match\"] = \"Unsure\"\n",
    "    spreadsheet.append(data_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet = pd.concat(spreadsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming header\n",
    "spreadsheet.columns = [\"_\".join(x.split(\" \")) for x in spreadsheet.columns]\n",
    "spreadsheet = spreadsheet.rename(columns={\"Resolution\":\"Resolution_Original\", \"Resolution_modified\":\"Resolution\", \"T/N\":\"Type\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create small spreadsheet if both no RNA and Panel data\n",
    "#spreadsheet[\"PANEL_renamed_bai_file\"] = [str(x) for x in spreadsheet[\"PANEL_renamed_bai_file\"]]\n",
    "#spreadsheet[\"RNA_fastq1\"] = [str(x) for x in spreadsheet[\"RNA_fastq1\"]]\n",
    "#spreadsheet_small = spreadsheet[(spreadsheet[\"PANEL_renamed_bai_file\"] != \"nan\") | (spreadsheet[\"RNA_fastq1\"] != \"nan\")]\n",
    "#new_index = [spreadsheet_small.iloc[i][\"Cell_Line_Starting_Sample\"] + \"_\" + str(x) for i,x in enumerate(spreadsheet_small.index)]\n",
    "#spreadsheet_small.index = new_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding standardized disease names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_map = pd.read_csv(data_location / \"all_diseases_manually_mapped - all_manuallly_mapped_by_Moony.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_map = disease_map.iloc[:, 0:3]\n",
    "disease_map[\"original_CCLF_disease_name\"] = [x.upper() for x in disease_map[\"original_CCLF_disease_name\"].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-100-edb5dc64b0b8>:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x[\"Disease_highest_level\"] = Highest_level_disease\n",
      "<ipython-input-100-edb5dc64b0b8>:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x[\"Disease_lowest_level\"] = Lowest_level_disease\n"
     ]
    }
   ],
   "source": [
    "merged = []\n",
    "for starting in spreadsheet[\"Cell_Line_Starting_Sample\"].unique():\n",
    "    x = spreadsheet[spreadsheet[\"Cell_Line_Starting_Sample\"] == starting]\n",
    "    y = x[\"Primary_Disease\"].unique()\n",
    "    y = [str(z) for z in y if str(z) != \"nan\"]\n",
    "    if len(y) > 1:\n",
    "        y = y[1]\n",
    "    \n",
    "    if len(y) == 0:\n",
    "        Highest_level_disease = \"nan\"\n",
    "        Lowest_level_disease = \"nan\"\n",
    "    else:\n",
    "        y = y[0].upper()\n",
    "        map_dis = disease_map[disease_map[\"original_CCLF_disease_name\"] == y]\n",
    "        if map_dis.empty:\n",
    "            Highest_level_disease = \"nan\"\n",
    "            Lowest_level_disease = \"nan\"\n",
    "        else:\n",
    "            Highest_level_disease = map_dis[\"Highest Level\"].tolist()[0]\n",
    "            Lowest_level_disease = map_dis[\"Lowest Level\"].tolist()[0]\n",
    "            \n",
    "    x[\"Disease_highest_level\"] = Highest_level_disease\n",
    "    x[\"Disease_lowest_level\"] = Lowest_level_disease\n",
    "    \n",
    "    merged.append(x)\n",
    "    \n",
    "spreadsheet = pd.concat(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create small spreadsheet if both no RNA and Panel data\n",
    "spreadsheet[\"PANEL_renamed_bai_file\"] = [str(x) for x in spreadsheet[\"PANEL_renamed_bai_file\"]]\n",
    "spreadsheet[\"RNA_fastq1\"] = [str(x) for x in spreadsheet[\"RNA_fastq1\"]]\n",
    "spreadsheet_small = spreadsheet[(spreadsheet[\"PANEL_renamed_bai_file\"] != \"nan\") | (spreadsheet[\"RNA_fastq1\"] != \"nan\")]\n",
    "new_index = [spreadsheet_small.iloc[i][\"Cell_Line_Starting_Sample\"] + \"_\" + str(x) for i,x in enumerate(spreadsheet_small.index)]\n",
    "spreadsheet_small.index = new_index\n",
    "\n",
    "new_index = [spreadsheet.iloc[i][\"Cell_Line_Starting_Sample\"] + \"_\" + str(x) for i,x in enumerate(spreadsheet.index)]\n",
    "spreadsheet.index = new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 592, 0: 2863})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(spreadsheet_small[\"Growth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_small = spreadsheet_small.replace({'Disease_lowest_level' : {'nan' : \"Unknown\", \n",
    "                                                     'Need more information (is it possible to know which case? so we can find out from collaborators)' : \"Unknown\", \n",
    "                                                     'Check the sample case (CCLF participant ID) to get a further info' : \"Unknown\",\n",
    "                                                     'MALIGNANT PERIPHERAL NERVE SHEATH TUMOR': 'Malignant Peripheral Nerve Sheath Tumor (C3798)'}})\n",
    "\n",
    "spreadsheet_small = spreadsheet_small.replace({'Disease_highest_level' : {'nan' : \"Unknown\", \n",
    "                                                     'Check the sample case (CCLF participant ID) to get a further info' : \"Unknown\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_small.to_csv(data_location / \"spreadsheet_small.tsv\", encoding = 'utf-8', sep='\\t')\n",
    "spreadsheet.to_csv(data_location / \"spreadsheet_compleet.tsv\", encoding = 'utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('Broadenv': virtualenv)",
   "language": "python",
   "name": "python37464bitbroadenvvirtualenv65599937babb4c6a9d24122a14fe3cd0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
